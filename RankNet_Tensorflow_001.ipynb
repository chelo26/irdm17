{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# RankNet MLP NN \n",
    "\n",
    "Issues: Learning well on training data but test accuracy not improving\n",
    "Hyperparamters:\n",
    "1) Sample in training data\n",
    "2) Epochs - currently high but training loss down to c.100\n",
    "3) Learning rate - currently OK for batch size\n",
    "4) n_hidden, n_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import tensorflow as tf \n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions to extrac the documents, query and rank information\n",
    "def extractFeatures(split):\n",
    "    features = []\n",
    "    for i in range(2, 138):\n",
    "        features.append(float(split[i].split(':')[1]))\n",
    "    # Convert to tuples:\n",
    "    return features\n",
    "\n",
    "def extractQueryData(split):\n",
    "    # Add tuples:\n",
    "    queryFeatures = split[1].split(':')[1]\n",
    "    return queryFeatures\n",
    "\n",
    "def readDataset(path):\n",
    "    print('Reading training data from file...')\n",
    "    with open(path, 'r') as file:\n",
    "        #k=0\n",
    "        features_list=[]\n",
    "        rank_list=[]\n",
    "        query_list=[]\n",
    "        for line in file:\n",
    "            split = line.split()\n",
    "            features_list.append(extractFeatures(split))\n",
    "            rank_list.append(int(split[0]))\n",
    "            query_list.append(extractQueryData(split))\n",
    "            #k+=1\n",
    "            #if k==100:\n",
    "            #    break\n",
    "    #print('Number of query ID %d' %(len(features_list)))\n",
    "    return features_list,rank_list,query_list\n",
    "\n",
    "\n",
    "# Normalisation:\n",
    "def normalize_features(features):\n",
    "    features=np.array(features)\n",
    "\n",
    "    # Substracting the mean:\n",
    "    mean_features = np.mean(features, axis=0)\n",
    "    features = features - mean_features\n",
    "\n",
    "    # Dividing by the std:\n",
    "    std_features = np.std(features, axis=0)\n",
    "    features = features / std_features\n",
    "    #print \"features normalized\"\n",
    "    return features\n",
    "\n",
    "\n",
    "# We put everything in a dictionary (key,value)= (query_id,[features,rank])\n",
    "def make_dictionary(features,ranks,queries):\n",
    "    dictio_quid=defaultdict(list)\n",
    "    for feature_vec,rank,query in zip(features,ranks,queries):\n",
    "        dictio_quid[query].append((feature_vec, rank))\n",
    "    return dictio_quid\n",
    "\n",
    "# Given a query ID, we separate on: [Xi,Xj,P_true] where P_true is either 0,0.5 or 1\n",
    "def get_pairs_features(dictio_quid_featsRank):\n",
    "    data = []\n",
    "    #k = 0\n",
    "    for key in dictio_quid_featsRank.keys():\n",
    "        # Temporary list of features,rank\n",
    "        temp_list = dictio_quid_featsRank[key]\n",
    "\n",
    "        for i in range(0, len(temp_list)):\n",
    "            X1 = temp_list[i][0]\n",
    "            rank1 = temp_list[i][1]\n",
    "            for j in range(i + 1, len(temp_list)):\n",
    "                X2 = temp_list[j][0]\n",
    "                rank2 = temp_list[j][1]\n",
    "\n",
    "                # Only look at queries with different id:\n",
    "                if (rank1 == rank2):\n",
    "                    data.append((X1, X2, 0.5))\n",
    "                if (rank1 > rank2):\n",
    "                    data.append((X1, X2, int(1)))\n",
    "                else:\n",
    "                    data.append((X1, X2, int(0)))\n",
    "        #k += 1\n",
    "        #if k % 100 == 0:\n",
    "           # print \"number of keys transformed: %d finished\" % int(k)\n",
    "    return data\n",
    "\n",
    "\n",
    "# Putting in the good format for tensorflow:\n",
    "\n",
    "\n",
    "def separate(data):\n",
    "    Xi = []\n",
    "    Xj = []\n",
    "    P_target = []\n",
    "    for instance in data:\n",
    "        Xi.append(instance[0])\n",
    "        Xj.append(instance[1])\n",
    "        P_target.append(instance[2])\n",
    "    return (np.array(Xi), np.array(Xj), np.array(P_target))\n",
    "\n",
    "# Sampling:\n",
    "def sampling_data(training_data, batch_size):\n",
    "    N = len(training_data)\n",
    "    indices = np.random.choice(N, batch_size)\n",
    "    #print (\"%d indices Selected\" ) % batch_size\n",
    "    return [training_data[i] for i in indices]\n",
    "\n",
    "# TensorFlow save model \n",
    "def save_model(session):\n",
    "    if not os.path.exists('./model/'):\n",
    "        os.mkdir('./model/')\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(session, './model/model.checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data from file...\n"
     ]
    }
   ],
   "source": [
    "#Read training data\n",
    "features,ranks,queries = readDataset('./MSLR-WEB10K/Fold1/train.txt')\n",
    "features=normalize_features(features)\n",
    "\n",
    "# Making a dictionary:\n",
    "dictio_quid = make_dictionary(features, ranks, queries)\n",
    "\n",
    "# Getting the paris of features vectors:\n",
    "training_data = get_pairs_features(dictio_quid)\n",
    "\n",
    "# Sampling:\n",
    "sampled_data = sampling_data(training_data, 10000)\n",
    "\n",
    "# Separating into array to put in tensorflow\n",
    "Xi, Xj, P_target = separate(sampled_data)\n",
    "P_target_r= np.reshape(P_target, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data from file...\n"
     ]
    }
   ],
   "source": [
    "# Validation set\n",
    "val_features, val_ranks, val_queries = readDataset('./MSLR-WEB10K/Fold1/vali.txt')\n",
    "val_features= normalize_features(val_features)\n",
    "val_dictio_quid = make_dictionary(val_features, val_ranks, val_queries)\n",
    "val_training_data = get_pairs_features(val_dictio_quid)\n",
    "val_sampled_data = sampling_data(val_training_data, 10000)\n",
    "val_Xi, val_Xj, val_P_target = separate(val_sampled_data)\n",
    "val_P_target_r= np.reshape(val_P_target, (-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_i = tf.placeholder(\"float\", [None, 136])\n",
    "x_j = tf.placeholder(\"float\", [None, 136])\n",
    "y_gold = tf.placeholder(\"float\", [None, 1])\n",
    "\n",
    "d_in = 136\n",
    "d_hidden = 500\n",
    "d_out = 1\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([d_in, d_hidden], mean= 0.01, stddev= 0.01))\n",
    "b1 = tf.Variable(tf.random_normal([d_hidden], mean= 0.01, stddev= 0.01))\n",
    "W2 = tf.Variable(tf.random_normal([d_hidden, d_out], mean= 0.01, stddev= 0.01))\n",
    "b2 = tf.Variable(tf.random_normal([d_out], mean= 0.01, stddev= 0.01))\n",
    "\n",
    "a1_i = tf.matmul(x_i, W1)+ b1\n",
    "z1_i = tf.sigmoid(a1_i)\n",
    "o_i = tf.matmul(z1_i, W2)+ b2\n",
    "\n",
    "a1_j = tf.matmul(x_j, W1)+ b1\n",
    "z1_j = tf.sigmoid(a1_j)\n",
    "o_j = tf.matmul(z1_j, W2)+ b2\n",
    "\n",
    "o_ij = o_i- o_j\n",
    "P_ij = tf.exp(o_ij)/ (1+ tf.exp(o_ij))\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y_gold* tf.log(tf.clip_by_value(P_ij, 1e-10,1.0)))\n",
    "optimiser = tf.train.GradientDescentOptimizer(0.015).minimize(cross_entropy)\n",
    "\n",
    "prediction = (tf.round((tf.clip_by_value(P_ij, 1e-10,0.99) * 3)+ 0.5) -1)/ 2 #Round 0.33-> 0, 0.34-> 0.5\n",
    "\n",
    "mistakes = tf.not_equal(y_gold, prediction)\n",
    "accuracy = 1- tf.reduce_mean(tf.cast(mistakes, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init train accuracy: 0.335\n",
      "Init test accuracy: 0.307\n",
      "Epoch 0 loss: 22689.6815788\n",
      "Epoch train/test accur: 0.346 0.307\n",
      "Final train accuracy: 0.346\n",
      "Final test accuracy: 0.307\n"
     ]
    }
   ],
   "source": [
    "# Train and test\n",
    "batch_sz= 30\n",
    "iter_= int(Xi.shape[0]/ batch_sz)\n",
    "epoch= 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    init_dict= {x_i: Xi[:1000],\n",
    "                x_j: Xj[:1000],\n",
    "                y_gold: P_target_r[:1000]}\n",
    "    print('Init train accuracy:', (sess.run(accuracy, feed_dict= init_dict)))\n",
    "    \n",
    "    test_dict= {x_i: val_Xi[1000:2000],\n",
    "                x_j: val_Xi[1000:2000],\n",
    "                y_gold: val_P_target_r[1000:2000]} \n",
    "    print('Init test accuracy:', (sess.run(accuracy, feed_dict= test_dict)))\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        e_loss= 0\n",
    "        \n",
    "        for i in range(iter_):\n",
    "            i_loss= 0\n",
    "            sta= i* batch_sz\n",
    "            end= (i+ 1)* batch_sz\n",
    "            iter_dict= {x_i: Xi[sta: end],\n",
    "                        x_j: Xj[sta: end],\n",
    "                        y_gold: P_target_r[sta: end]}\n",
    "            sess.run(optimiser, feed_dict= iter_dict)\n",
    "            e_loss+= sess.run(cross_entropy, feed_dict= iter_dict)\n",
    "        \n",
    "        if e% (epoch/ 10)== 0:\n",
    "            print('Epoch', e, 'loss:', e_loss)\n",
    "            print('Epoch train/test accur:', (sess.run(accuracy, feed_dict= init_dict)), (sess.run(accuracy, feed_dict= test_dict)))\n",
    "        \n",
    "    print('Final train accuracy:', (sess.run(accuracy, feed_dict= init_dict)))\n",
    "    print('Final test accuracy:', (sess.run(accuracy, feed_dict= test_dict)))\n",
    "    \n",
    "    # TensorFlow save model\n",
    "    if not os.path.exists('./model/'):\n",
    "        os.mkdir('./model/')\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, './model/model.checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.30699998"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TensorFlow restore model\n",
    "with tf.Session() as sess:\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, './model/model.checkpoint')\n",
    "    \n",
    "    test_dict= {x_i: val_Xi[1000:2000],\n",
    "                x_j: val_Xi[1000:2000],\n",
    "                y_gold: val_P_target_r[1000:2000]} \n",
    "    test_predicted = sess.run(prediction, feed_dict=test_dict)\n",
    "    test_accuracy = sess.run(accuracy, feed_dict=test_dict)\n",
    "\n",
    "test_accuracy"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
