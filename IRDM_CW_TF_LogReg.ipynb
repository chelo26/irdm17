{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import math\n",
    "# import pandas\n",
    "from optparse import OptionParser\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import ensemble\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from multiprocessing import Pool\n",
    "from itertools import chain\n",
    "import time\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions to extrac the documents, query and rank information\n",
    "def extractFeatures(split):\n",
    "    features = []\n",
    "    for i in range(2, 138):\n",
    "        features.append(float(split[i].split(':')[1]))\n",
    "    # Convert to tuples:\n",
    "    return features\n",
    "\n",
    "def extractQueryData(split):\n",
    "    # Add tuples:\n",
    "    queryFeatures = split[1].split(':')[1]\n",
    "    return queryFeatures\n",
    "\n",
    "def readDataset(path):\n",
    "    dictio_quid= defaultdict(list)\n",
    "    features_list=[]\n",
    "    rank_list=[]\n",
    "    print('Reading training data from file...')\n",
    "    k=0\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Getting features:\n",
    "            split = line.split()\n",
    "            features=extractFeatures(split)\n",
    "            features_list.append(features)\n",
    "\n",
    "            # Getting the query:\n",
    "            query = int(extractQueryData(split))\n",
    "            #print \"query: \"+str(query)\n",
    "            # Getting rank:\n",
    "            rank = int(split[0])\n",
    "            rank_list.append(rank)\n",
    "\n",
    "            # Feeding dictionary:\n",
    "            dictio_quid[query].append((features, rank))\n",
    "            k+=1\n",
    "            #if k==10:\n",
    "            #   break\n",
    "    print('Number of query ID %d' %(len(features_list)))\n",
    "    return np.array(features_list), np.array(rank_list), dictio_quid\n",
    "\n",
    "# Normalisation:\n",
    "def normalize_features(features):\n",
    "    features=np.array(features)\n",
    "\n",
    "    # Substracting the mean:\n",
    "    mean_features = np.mean(features, axis=0)\n",
    "    features = features - mean_features\n",
    "\n",
    "    # Dividing by the std:\n",
    "    std_features = np.std(features, axis=0)\n",
    "    features = features / std_features\n",
    "    #print \"features normalized\"\n",
    "    return features, mean_features, std_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data from file...\n",
      "Number of query ID 723412\n"
     ]
    }
   ],
   "source": [
    "features, ranks, queries= readDataset('LEMUR/MSLR-WEB10K/Fold1/train.txt')\n",
    "features, _, _= normalize_features(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data from file...\n",
      "Number of query ID 241521\n"
     ]
    }
   ],
   "source": [
    "tes_features, tes_ranks, tes_queries= readDataset('LEMUR/MSLR-WEB10K/Fold1/test.txt')\n",
    "tes_features, mean_Xval, std_Xval= normalize_features(tes_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ranks_r = np.asarray(ranks).reshape(-1, 1)\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(ranks_r)\n",
    "ranks_enc = enc.transform(ranks_r).toarray()\n",
    "queries_r = np.asarray(queries).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tes_ranks_r = np.asarray(tes_ranks).reshape(-1, 1)\n",
    "tes_ranks_enc = enc.transform(tes_ranks_r).toarray()\n",
    "tes_queries_r = np.asarray(tes_queries).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reorder_dictio(dictio_eval):\n",
    "    for key in dictio_eval.keys():\n",
    "        #dictio_eval[key]=sorted(dictio_eval[key], reverse=True,key=lambda tup: (tup[1], tup[0]))\n",
    "        dictio_eval[key] = sorted(dictio_eval[key], reverse=True, key=lambda tup: tup[1])\n",
    "    return dictio_eval\n",
    "\n",
    "def dcg_score(y_true, y_score, k=10, gains=\"exponential\"):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "\n",
    "    if gains == \"exponential\":\n",
    "        gains = 2 ** y_true - 1\n",
    "    elif gains == \"linear\":\n",
    "        gains = y_true\n",
    "    else:\n",
    "        raise ValueError(\"Invalid gains option.\")\n",
    "\n",
    "    # highest rank is 1 so +2 instead of +1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10, gains=\"exponential\"):\n",
    "    best = dcg_score(y_true, y_true, k, gains)\n",
    "    actual = dcg_score(y_true, y_score, k, gains)\n",
    "    return actual / best\n",
    "\n",
    "def separate(relevance_tuple):\n",
    "    y_true=[]\n",
    "    y_pred = []\n",
    "    for tup in relevance_tuple:\n",
    "        y_pred.append(tup[0])\n",
    "        y_true.append(tup[1])\n",
    "    return y_true,y_pred\n",
    "\n",
    "def ndcg(dictio_eval):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    b= reorder_dictio(dictio_eval)\n",
    "    new_b=[]\n",
    "    for qid in b.keys():\n",
    "        r_true=[]\n",
    "        r_pred=[]\n",
    "        for i,j in zip(pd.DataFrame(b[qid])[0], pd.DataFrame(b[qid])[1]):\n",
    "            r_pred.append(i)\n",
    "            r_true.append(j)\n",
    "        score=ndcg_score(r_true,r_pred)    \n",
    "        #new_b[qid]= [r_true, r_pred]\n",
    "        new_b.append(score)\n",
    "    return new_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_bins(dictio_eval):\n",
    "    #Get the bins:\n",
    "    all_tuples=[]\n",
    "    for key in dictio_eval.keys():\n",
    "        all_tuples.append(dictio_eval[key])\n",
    "    all_tuples=list(chain.from_iterable(all_tuples))\n",
    "    all_scores=[i[0] for i in all_tuples]\n",
    "    all_true=[i[1] for i in all_tuples]\n",
    "    number_bins=4#len(set(all_true))\n",
    "    bins = np.histogram(all_scores, bins=number_bins, range=None, normed=False, weights=None)[1]\n",
    "    return bins\n",
    "\n",
    "def assign_to_bin(list_score,bins):\n",
    "    # Get bins\n",
    "    #bins = np.histogram(list_score, bins=number_bins, range=None, normed=False, weights=None)[1]\n",
    "    inds = np.digitize(list_score, bins,right=True)\n",
    "    return inds\n",
    "\n",
    "# Functions to calculate ERR:\n",
    "GAMMA=0.450\n",
    "\n",
    "# Functions to calculate ERR:\n",
    "def get_proba(list_tuples):\n",
    "    list_proba=[]\n",
    "    for r_pred,r_true in list_tuples:\n",
    "        proba = (np.power(2,r_pred)-1)/np.power(2,4)\n",
    "        list_proba.append(proba)\n",
    "    return list_proba\n",
    "\n",
    "def get_ERR(list_proba,n=10,gamma=0.5):\n",
    "    r=2\n",
    "    err = list_proba[0]\n",
    "    last_proba=1\n",
    "    for i in xrange(1,len(list_proba)):\n",
    "        actual_proba=list_proba[i]\n",
    "        previous_proba=(1-list_proba[i-1])*last_proba\n",
    "        #print proba\n",
    "        stop_proba=actual_proba*previous_proba\n",
    "        err+=stop_proba/r\n",
    "        last_proba=previous_proba\n",
    "        r+=1\n",
    "    return err\n",
    "\n",
    "def ERR(dictio_eval,n=10,gamma=0.5):\n",
    "    list_ERR=[]\n",
    "    for key in dictio_eval.keys():\n",
    "        list_tuples=dictio_eval[key]\n",
    "        list_proba=get_proba(list_tuples)\n",
    "        err_result=get_ERR(list_proba,n,gamma)\n",
    "        list_ERR.append(err_result)\n",
    "    return list_ERR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, dictio_val, mean_Xval, std_Xval):\n",
    "    dictio_evaluation = defaultdict(list)\n",
    "\n",
    "    for key in dictio_val.keys():\n",
    "        temp_list = dictio_val[key]\n",
    "        for features_vec, relevance in temp_list:\n",
    "            # Features:\n",
    "            features_norm = (np.array(features_vec) - mean_Xval) / std_Xval\n",
    "            features_norm = features_norm.reshape(1,-1)\n",
    "            #features_norm = features_norm.reshape(-1,1)\n",
    "\n",
    "            # Prediction:\n",
    "            prediction = model.predict(features_norm)\n",
    "\n",
    "            # Dictionary:\n",
    "            dictio_evaluation[key].append((prediction[0], relevance))\n",
    "\n",
    "            # print features_vec,relevance\n",
    "\n",
    "    return dictio_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TF_LogReg():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.self = self\n",
    "        \n",
    "    def initilise_model(self):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.x_i = tf.placeholder(\"float\", [None, 136])\n",
    "        self.y_i = tf.placeholder(\"float\", [None, 5])\n",
    "        self.qid = tf.placeholder(\"float\", [None, 1])\n",
    "\n",
    "        d_in = 136\n",
    "        d_out = 5\n",
    "\n",
    "        self.W1 = tf.Variable(tf.random_normal([d_in, d_out], mean= 0.01, stddev= 0.01))\n",
    "        self.b1 = tf.Variable(tf.random_normal([d_out], mean= 0.01, stddev= 0.01))\n",
    "\n",
    "        self.a1 = tf.matmul(self.x_i, self.W1)+ self.b1\n",
    "        self.y_hat = self.a1\n",
    "\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels= self.y_i, logits=self.y_hat))\n",
    "        \n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.starter_learning_rate = 0.0001\n",
    "        self.learning_rate = tf.train.exponential_decay(self.starter_learning_rate, self.global_step,\n",
    "                                                   100000, 0.96, staircase=True)\n",
    "        self.optimiser = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss) \n",
    "        \n",
    "        self.prediction = tf.argmax(self.y_hat, axis=1)\n",
    "        self.correct = tf.argmax(self.y_i, axis=1)\n",
    "        self.mistakes = tf.not_equal(self.correct, self.prediction)\n",
    "        self.accuracy = 1- tf.reduce_mean(tf.cast(self.mistakes, tf.float32))\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "\n",
    "    def initilise_model_linear(self):\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        \n",
    "        self.x_i = tf.placeholder(\"float\", [None, 136])\n",
    "        self.y_i = tf.placeholder(\"float\", [None, 1])\n",
    "        self.qid = tf.placeholder(\"float\", [None, 1])\n",
    "\n",
    "        d_in = 136\n",
    "        d_out = 1\n",
    "\n",
    "        self.W1 = tf.Variable(tf.random_normal([d_in, d_out], mean= 0.01, stddev= 0.01))\n",
    "        self.b1 = tf.Variable(tf.random_normal([d_out], mean= 0.01, stddev= 0.01))\n",
    "\n",
    "        self.a1 = tf.matmul(self.x_i, self.W1)+ self.b1\n",
    "        self.y_hat = tf.nn.sigmoid(self.a1)* 4\n",
    "\n",
    "        self.correct = tf.argmax(self.y_i, axis=1)\n",
    "        self.loss = tf.losses.mean_squared_error(self.y_hat, self.y_i)\n",
    "        \n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        self.starter_learning_rate = 0.00001\n",
    "        self.learning_rate = tf.train.exponential_decay(self.starter_learning_rate, self.global_step,\n",
    "                                                   100000, 0.96, staircase=True)\n",
    "        self.optimiser = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.loss) \n",
    "        \n",
    "        self.prediction = tf.round(self.y_hat)\n",
    "        self.prediction_d = tf.cast(self.prediction, tf.int64)\n",
    "        \n",
    "        self.correct = tf.argmax(self.y_i, axis=1)\n",
    "        self.mistakes = tf.not_equal(self.y_i, self.prediction_d)\n",
    "        self.accuracy = 1- tf.reduce_mean(tf.cast(self.mistakes, tf.float32))\n",
    "        \n",
    "        self.saver = tf.train.Saver()\n",
    "        \n",
    "    def train_all(self, x_train, y_train, epoch):\n",
    "        print('Train with all data, unbatched')\n",
    "        \n",
    "        self.train_dict= {self.x_i: x_train,\n",
    "                          self.y_i: y_train} \n",
    "        print('Initial training acc %s' % (sess.run(self.accuracy, feed_dict= self.train_dict)))\n",
    "        for i in range(epoch):\n",
    "            sess.run(self.optimiser, feed_dict= self.train_dict)\n",
    "        print('Final training acc %s' % (sess.run(self.accuracy, feed_dict= self.train_dict)))\n",
    "        \n",
    "    def train_full(self, x_train, y_train, epoch, batch_sz, x_valid= None, y_valid= None):\n",
    "\n",
    "        self.iter_= int(x_train.shape[0]/ batch_sz)\n",
    "        \n",
    "        self.train_dict= {self.x_i: x_train,\n",
    "                          self.y_i: y_train} \n",
    "    \n",
    "        print('Training: Iters: %s. Epoch:  %s' % (self.iter_, epoch))\n",
    "        print('Initial training acc %s' % (sess.run(self.accuracy, feed_dict= self.train_dict)))\n",
    "        \n",
    "        for e in range(epoch):\n",
    "            e_loss= 0\n",
    "                \n",
    "            for i in range(self.iter_):\n",
    "                iter_dict= {self.x_i: x_train[(i* batch_sz):((i+ 1)* batch_sz)],\n",
    "                            self.y_i: y_train[(i* batch_sz):((i+ 1)* batch_sz)]}\n",
    "                \n",
    "                sess.run(self.optimiser, feed_dict= iter_dict)\n",
    "                e_loss+= sess.run(self.loss, feed_dict= iter_dict)\n",
    "                \n",
    "                #if i %300 == 0:\n",
    "                #    print('Epoch %s, iter %s, loss %s' % (e, i, sess.run(self.loss, feed_dict= iter_dict)))\n",
    "                \n",
    "            print('Epoch %s, loss %s' % (e, sess.run(self.loss, feed_dict= iter_dict)))\n",
    "        print('Training accuracy %s' % (sess.run(self.accuracy, feed_dict= self.train_dict)))\n",
    "        \n",
    "    def predict(self, x_valid):\n",
    "        valid_dict= {self.x_i: x_valid}  \n",
    "        predict_1 = (sess.run(self.prediction, feed_dict= valid_dict))\n",
    "        return predict_1\n",
    "        \n",
    "    def test_pointwise(self, x_valid, y_valid):\n",
    "        self.valid_dict= {self.x_i: x_valid,\n",
    "                          self.y_i: y_valid} \n",
    "        val_accuracy = (sess.run(self.accuracy, feed_dict= self.valid_dict))\n",
    "        print('Test accuracy: %s' % (val_accuracy))\n",
    "\n",
    "    def init_disp_var(self, x_array, y_array, iter_= 50):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        self.show_dict= {self.x_i: x_array[:10],\n",
    "                         self.y_i: y_array[:10]} \n",
    "        print('Pre op:', sess.run(self.accuracy, feed_dict= self.show_dict))\n",
    "        for i in range(iter_):\n",
    "            sess.run(self.optimiser, feed_dict= self.show_dict)\n",
    "        print('Post op:', sess.run(self.y_hat, feed_dict= self.show_dict))\n",
    "        \n",
    "    def save_model(self, sess, dir_name):\n",
    "        print('Saving model...')\n",
    "        if not os.path.exists('./'+ dir_name+ '/'):\n",
    "            os.mkdir('./'+ dir_name+ '/')\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, './'+ dir_name+ '/model.checkpoint')\n",
    "        print('Model saved')\n",
    "\n",
    "    def load_model(self, sess, dir_name):\n",
    "        print('Loading model...')\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, './'+ dir_name+ '/model.checkpoint')\n",
    "        print('Model loaded')\n",
    "        return sess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Start---\n",
      "1. Initialise\n",
      "Loading model...\n",
      "Model loaded\n",
      "Test accuracy: 0.527002\n",
      "Pre-train nDGC tes: 0.268081670047, ERR: 0.109273583201\n"
     ]
    }
   ],
   "source": [
    "print('---Start---')\n",
    "\n",
    "model1= TF_LogReg()\n",
    "model1.initilise_model()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    print('1. Initialise')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    model1.load_model(sess, '20170417_LogReg')\n",
    "    \n",
    "    model1.test_pointwise(tes_features, tes_ranks_enc)\n",
    "    \n",
    "    pred_dictio_eval = evaluate(model1, tes_queries, mean_Xval, std_Xval)\n",
    "    nDGC_eval_lst = ndcg(pred_dictio_eval)\n",
    "    ERR_eval_lst = ERR(pred_dictio_eval)\n",
    "    print('Pre-train nDGC tes: %s, ERR: %s' % (np.nanmean(nDGC_eval_lst), np.nanmean(ERR_eval_lst)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Start---\n",
      "1. Initialise\n",
      "Test accuracy: 0.240575\n",
      "Pre-train nDGC tes: 0.204874215314, ERR: 0.49609029241\n",
      "2. Train\n",
      "Training: Iters: 24113. Epoch:  1\n",
      "Initial training acc 0.242126\n",
      "Epoch 0, loss 1.00344\n",
      "Training accuracy 0.554955\n",
      "3. Test\n",
      "Test accuracy: 0.541965\n",
      "Post-train nDGC tes: 0.252887304128, ERR: 0.113063978213\n",
      "---End---\n"
     ]
    }
   ],
   "source": [
    "print('---Start---')\n",
    "\n",
    "model1= TF_LogReg()\n",
    "model1.initilise_model()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    print('1. Initialise')\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    model1.load_model(sess, '20170419_LogReg')\n",
    "    \n",
    "    model1.test_pointwise(tes_features, tes_ranks_enc)\n",
    "    \n",
    "    pred_dictio_eval = evaluate(model1, tes_queries, mean_Xval, std_Xval)\n",
    "    nDGC_eval_lst = ndcg(pred_dictio_eval)\n",
    "    ERR_eval_lst = ERR(pred_dictio_eval)\n",
    "    print('Pre-train nDGC tes: %s, ERR: %s' % (np.nanmean(nDGC_eval_lst), np.nanmean(ERR_eval_lst)))\n",
    "    \n",
    "    print('2. Train')\n",
    "    model1.train_full(features, ranks_enc, 1, 30)\n",
    "    \n",
    "    print('3. Test')\n",
    "    model1.test_pointwise(tes_features, tes_ranks_enc)\n",
    "    \n",
    "    pred_dictio_eval = evaluate(model1, tes_queries, mean_Xval, std_Xval)\n",
    "    nDGC_eval_lst = ndcg(pred_dictio_eval)\n",
    "    ERR_eval_lst = ERR(pred_dictio_eval)\n",
    "    \n",
    "    print('Post-train nDGC tes: %s, ERR: %s' % (np.nanmean(nDGC_eval_lst), np.nanmean(ERR_eval_lst)))\n",
    "    model1.save_model(sess, '20170419_LogReg')\n",
    "    \n",
    "print('---End---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1\n",
      "0  0  4\n",
      "1  0  4\n",
      "2  2  3\n",
      "3  0  3\n",
      "4  1  2\n",
      "5  0  2\n",
      "6  0  1\n",
      "7  0  1\n",
      "8  0  1\n",
      "9  0  1\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(pred_dictio_eval[58])[:10])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
