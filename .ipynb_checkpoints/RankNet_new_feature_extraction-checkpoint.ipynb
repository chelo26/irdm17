{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from collections import defaultdict\n",
    "% pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Functions to extract pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions to extrac the documents, query and rank information\n",
    "# We put everything in a dictionary (key,value)= (query_id,[features,rank])\n",
    "def extractFeatures(split):\n",
    "    features = []\n",
    "    for i in xrange(2, 138):\n",
    "        features.append(float(split[i].split(':')[1]))\n",
    "    return features\n",
    "\n",
    "def extractQueryData(split):\n",
    "    queryFeatures = [split[1].split(':')[1]]\n",
    "    return queryFeatures\n",
    "\n",
    "def readDataset(path):\n",
    "    dictio_quid= defaultdict(list)\n",
    "    print('Reading training data from file...')\n",
    "\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            split = line.split()\n",
    "            rank=int(split[0])\n",
    "            features=extractFeatures(split)\n",
    "            query=extractQueryData(split)\n",
    "            dictio_quid[query[0]].append((features,rank))\n",
    "\n",
    "    print('Number of query ID %d' %(len(dictio_quid)))\n",
    "    return dictio_quid\n",
    "\n",
    "# Given a query ID, we separate on: [Xi,Xj,P_true] where P_true is either 0,0.5 or 1\n",
    "def get_pairs_features(dictio_quid_featsRank):\n",
    "    data=[]\n",
    "    k=0\n",
    "    for key in dictio_quid_featsRank.keys():\n",
    "        # Temporary list of features,rank\n",
    "        temp_list = dictio_quid_featsRank[key]\n",
    "\n",
    "        for i in xrange(0, len(temp_list)):\n",
    "            X1 = temp_list[i][0]\n",
    "            rank1 = temp_list[i][1]\n",
    "            for j in xrange(i + 1, len(temp_list)):\n",
    "                X2=temp_list[j][0]\n",
    "                rank2=temp_list[j][1]\n",
    "\n",
    "                # Only look at queries with different id:\n",
    "                if (rank1==rank2):\n",
    "                    break\n",
    "                #    data.append((X1,X2,0.5))\n",
    "                if (rank1>rank2):\n",
    "                    data.append((X1,X2,int(1)))\n",
    "                else:\n",
    "                    data.append((X1, X2,int(0)))\n",
    "        k+=1\n",
    "        if k%100==0:\n",
    "            print \"number of keys transformed: %d finished\"%int(k)\n",
    "    return data\n",
    "\n",
    "def sampling_data(training_data,batch_size):\n",
    "    N=len(training_data)\n",
    "    indices = np.random.choice(N, batch_size)\n",
    "    print \"%d indices Selected\"%batch_size\n",
    "    return [training_data[i] for i in indices]\n",
    "    \n",
    "def separate(data):\n",
    "    Xi=[]\n",
    "    Xj=[]\n",
    "    P_target=[]\n",
    "    for instance in data:\n",
    "        Xi.append(instance[0])\n",
    "        Xj.append(instance[1])\n",
    "        P_target.append(instance[2])\n",
    "    return (np.array(Xi),np.array(Xj),np.array(P_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data from file...\n",
      "Number of query ID 6000\n",
      "number of keys transformed: 100 finished\n",
      "number of keys transformed: 200 finished\n",
      "number of keys transformed: 300 finished\n",
      "number of keys transformed: 400 finished\n",
      "number of keys transformed: 500 finished\n",
      "number of keys transformed: 600 finished\n",
      "number of keys transformed: 700 finished\n",
      "number of keys transformed: 800 finished\n",
      "number of keys transformed: 900 finished\n",
      "number of keys transformed: 1000 finished\n",
      "number of keys transformed: 1100 finished\n",
      "number of keys transformed: 1200 finished\n",
      "number of keys transformed: 1300 finished\n",
      "number of keys transformed: 1400 finished\n",
      "number of keys transformed: 1500 finished\n",
      "number of keys transformed: 1600 finished\n",
      "number of keys transformed: 1700 finished\n",
      "number of keys transformed: 1800 finished\n",
      "number of keys transformed: 1900 finished\n",
      "number of keys transformed: 2000 finished\n",
      "number of keys transformed: 2100 finished\n",
      "number of keys transformed: 2200 finished\n",
      "number of keys transformed: 2300 finished\n",
      "number of keys transformed: 2400 finished\n",
      "number of keys transformed: 2500 finished\n",
      "number of keys transformed: 2600 finished\n",
      "number of keys transformed: 2700 finished\n",
      "number of keys transformed: 2800 finished\n",
      "number of keys transformed: 2900 finished\n",
      "number of keys transformed: 3000 finished\n",
      "number of keys transformed: 3100 finished\n",
      "number of keys transformed: 3200 finished\n",
      "number of keys transformed: 3300 finished\n",
      "number of keys transformed: 3400 finished\n",
      "number of keys transformed: 3500 finished\n",
      "number of keys transformed: 3600 finished\n",
      "number of keys transformed: 3700 finished\n",
      "number of keys transformed: 3800 finished\n",
      "number of keys transformed: 3900 finished\n",
      "number of keys transformed: 4000 finished\n",
      "number of keys transformed: 4100 finished\n",
      "number of keys transformed: 4200 finished\n",
      "number of keys transformed: 4300 finished\n",
      "number of keys transformed: 4400 finished\n",
      "number of keys transformed: 4500 finished\n",
      "number of keys transformed: 4600 finished\n",
      "number of keys transformed: 4700 finished\n",
      "number of keys transformed: 4800 finished\n",
      "number of keys transformed: 4900 finished\n",
      "number of keys transformed: 5000 finished\n",
      "number of keys transformed: 5100 finished\n",
      "number of keys transformed: 5200 finished\n",
      "number of keys transformed: 5300 finished\n",
      "number of keys transformed: 5400 finished\n",
      "number of keys transformed: 5500 finished\n",
      "number of keys transformed: 5600 finished\n",
      "number of keys transformed: 5700 finished\n",
      "number of keys transformed: 5800 finished\n",
      "number of keys transformed: 5900 finished\n",
      "number of keys transformed: 6000 finished\n"
     ]
    }
   ],
   "source": [
    "#Read training data\n",
    "dictio_query = readDataset('./MSLR-WEB10K/Fold1/train.txt')\n",
    "#dictio_query_val = readDataset('./MSLR-WEB10K/Fold1/vali.txt')\n",
    "# Extract document pairs\n",
    "training_data=get_pairs_features(dictio_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150000 indices Selected\n"
     ]
    }
   ],
   "source": [
    "# Sampling data:\n",
    "n_samples=150000\n",
    "data=sampling_data(training_data,n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Constant, variables and place holders:\n",
    "nDim=136\n",
    "N=len(data)\n",
    "first_layer=40\n",
    "output_layer=1\n",
    "A = tf.placeholder(tf.float32, [None, nDim])\n",
    "B = tf.placeholder(tf.float32, [None, nDim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "P_AB = tf.placeholder(tf.float32, [None, output_layer])\n",
    "P_true = tf.placeholder(tf.float32, [None, output_layer]) # float32 before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Weights for the first layer to hidden layer:\n",
    "weights1 = tf.Variable(tf.random_normal([nDim, first_layer]))\n",
    "biases1 = tf.Variable(tf.random_normal([first_layer]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hidden Layer nodes:\n",
    "hiddenA = tf.matmul(A, weights1) + biases1\n",
    "hiddenB = tf.matmul(B, weights1) + biases1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Activations hidden layer\n",
    "act_hiddenA = tf.nn.sigmoid(hiddenA)\n",
    "act_hiddenB = tf.nn.sigmoid(hiddenB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Weights from hidden layer to output layer:\n",
    "weights2 = tf.Variable(tf.random_normal([first_layer,output_layer]))\n",
    "biases2 = tf.Variable(tf.random_normal([output_layer]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Output layer:\n",
    "outputA = tf.matmul(act_hiddenA, weights2) + biases2\n",
    "outputB = tf.matmul(act_hiddenB, weights2) + biases2\n",
    "\n",
    "Oi = tf.nn.sigmoid(outputA)\n",
    "Oj = tf.nn.sigmoid(outputB)\n",
    "Oij=Oi-Oj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Probability:\n",
    "Pij=tf.exp(Oij)/(1+tf.exp(Oij))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cross entropy and cost:\n",
    "cross_entropy = tf.reduce_sum(-P_true*Oij + tf.log(1+tf.exp(Oij)))\n",
    "#cross_entropy = -tf.reduce_sum(tf.log(Pij))\n",
    "\n",
    "#cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=Pij,labels=P_true)\n",
    "#cost = tf.reduce_mean(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optimizer:\n",
    "#optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(cost)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.00001).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start session:\n",
    "batch_size = 100\n",
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xi,Xj,P_target=separate(data)\n",
    "P_target=P_target.reshape(P_target.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def optimize(num_iterations):\n",
    "    for i in range(num_iterations):\n",
    "        # Get a batch of training examples.\n",
    "        # x_batch now holds a batch of images and\n",
    "        # y_true_batch are the true labels for those images.\n",
    "#        A_batch, y_true_batch = data.train.next_batch(batch_size)\n",
    "        indices = np.random.choice(len(data), n_samples)\n",
    "        A_batch, B_batch, target_batch = Xi[indices], Xj[indices] , P_target[indices]\n",
    "        \n",
    "        # Put the batch into a dict with the proper names\n",
    "        # for placeholder variables in the TensorFlow graph.\n",
    "        # Note that the placeholder for y_true_cls is not set\n",
    "        # because it is not used during training.\n",
    "        #\n",
    "        #feed_dict_train = {A: data1,B:data2, P_true: target} --> working\n",
    "        feed_dict_train = {A: A_batch,B:B_batch, P_true: P_target} \n",
    "        # Run the optimizer using this batch of training data.\n",
    "        # TensorFlow assigns the variables in feed_dict_train\n",
    "        # to the placeholder variables and then runs the optimizer.\n",
    "        session.run(optimizer, feed_dict=feed_dict_train)\n",
    "        #oi = session.run(Oi, feed_dict= feed_dict_train)\n",
    "        #oj = session.run(Oj, feed_dict= feed_dict_train)\n",
    "        #oij = session.run(Oij, feed_dict= feed_dict_train)\n",
    "        #pij = session.run(Pij, feed_dict= feed_dict_train)\n",
    "        c_e = session.run(cross_entropy, feed_dict= feed_dict_train)\n",
    "        #error = session.run(cost, feed_dict= feed_dict_train)\n",
    "        print('Epoch', i, \"loss:  \",c_e)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Epoch', 0, 'loss:  ', 104300.79)\n",
      "('Epoch', 1, 'loss:  ', 104275.17)\n",
      "('Epoch', 2, 'loss:  ', 104239.87)\n",
      "('Epoch', 3, 'loss:  ', 104252.09)\n",
      "('Epoch', 4, 'loss:  ', 104221.68)\n",
      "('Epoch', 5, 'loss:  ', 104193.38)\n",
      "('Epoch', 6, 'loss:  ', 104168.0)\n",
      "('Epoch', 7, 'loss:  ', 104117.72)\n",
      "('Epoch', 8, 'loss:  ', 104163.95)\n",
      "('Epoch', 9, 'loss:  ', 104112.41)\n",
      "('Epoch', 10, 'loss:  ', 104172.22)\n",
      "('Epoch', 11, 'loss:  ', 104154.53)\n",
      "('Epoch', 12, 'loss:  ', 104124.66)\n",
      "('Epoch', 13, 'loss:  ', 104126.11)\n",
      "('Epoch', 14, 'loss:  ', 104121.61)\n",
      "('Epoch', 15, 'loss:  ', 104102.13)\n",
      "('Epoch', 16, 'loss:  ', 104101.17)\n",
      "('Epoch', 17, 'loss:  ', 104081.03)\n",
      "('Epoch', 18, 'loss:  ', 104059.56)\n",
      "('Epoch', 19, 'loss:  ', 104067.92)\n",
      "('Epoch', 20, 'loss:  ', 104088.89)\n",
      "('Epoch', 21, 'loss:  ', 104082.86)\n",
      "('Epoch', 22, 'loss:  ', 104063.69)\n",
      "('Epoch', 23, 'loss:  ', 104054.72)\n",
      "('Epoch', 24, 'loss:  ', 104067.52)\n",
      "('Epoch', 25, 'loss:  ', 104077.92)\n",
      "('Epoch', 26, 'loss:  ', 104062.02)\n",
      "('Epoch', 27, 'loss:  ', 104063.95)\n",
      "('Epoch', 28, 'loss:  ', 104046.05)\n",
      "('Epoch', 29, 'loss:  ', 104070.28)\n",
      "('Epoch', 30, 'loss:  ', 104076.98)\n",
      "('Epoch', 31, 'loss:  ', 104067.91)\n",
      "('Epoch', 32, 'loss:  ', 104069.8)\n",
      "('Epoch', 33, 'loss:  ', 104055.69)\n",
      "('Epoch', 34, 'loss:  ', 104059.0)\n",
      "('Epoch', 35, 'loss:  ', 103997.86)\n",
      "('Epoch', 36, 'loss:  ', 104037.26)\n",
      "('Epoch', 37, 'loss:  ', 104036.04)\n",
      "('Epoch', 38, 'loss:  ', 104032.8)\n",
      "('Epoch', 39, 'loss:  ', 104007.48)\n",
      "('Epoch', 40, 'loss:  ', 104017.45)\n",
      "('Epoch', 41, 'loss:  ', 103990.9)\n",
      "('Epoch', 42, 'loss:  ', 104010.86)\n",
      "('Epoch', 43, 'loss:  ', 104076.15)\n",
      "('Epoch', 44, 'loss:  ', 104077.23)\n",
      "('Epoch', 45, 'loss:  ', 104008.26)\n",
      "('Epoch', 46, 'loss:  ', 104009.02)\n",
      "('Epoch', 47, 'loss:  ', 104041.24)\n",
      "('Epoch', 48, 'loss:  ', 104022.92)\n",
      "('Epoch', 49, 'loss:  ', 104009.16)\n",
      "('Epoch', 50, 'loss:  ', 104023.5)\n",
      "('Epoch', 51, 'loss:  ', 104016.17)\n",
      "('Epoch', 52, 'loss:  ', 104001.7)\n",
      "('Epoch', 53, 'loss:  ', 104059.03)\n",
      "('Epoch', 54, 'loss:  ', 103989.99)\n",
      "('Epoch', 55, 'loss:  ', 104005.0)\n",
      "('Epoch', 56, 'loss:  ', 104016.55)\n",
      "('Epoch', 57, 'loss:  ', 104024.66)\n",
      "('Epoch', 58, 'loss:  ', 104017.79)\n",
      "('Epoch', 59, 'loss:  ', 104021.95)\n",
      "('Epoch', 60, 'loss:  ', 104017.16)\n",
      "('Epoch', 61, 'loss:  ', 104024.72)\n",
      "('Epoch', 62, 'loss:  ', 104001.2)\n",
      "('Epoch', 63, 'loss:  ', 104025.42)\n",
      "('Epoch', 64, 'loss:  ', 104005.41)\n",
      "('Epoch', 65, 'loss:  ', 104000.34)\n",
      "('Epoch', 66, 'loss:  ', 104020.16)\n",
      "('Epoch', 67, 'loss:  ', 104031.2)\n",
      "('Epoch', 68, 'loss:  ', 104004.5)\n",
      "('Epoch', 69, 'loss:  ', 104005.2)\n",
      "('Epoch', 70, 'loss:  ', 104005.36)\n",
      "('Epoch', 71, 'loss:  ', 104015.52)\n",
      "('Epoch', 72, 'loss:  ', 104016.43)\n",
      "('Epoch', 73, 'loss:  ', 104009.18)\n",
      "('Epoch', 74, 'loss:  ', 104003.12)\n",
      "('Epoch', 75, 'loss:  ', 103987.2)\n",
      "('Epoch', 76, 'loss:  ', 103993.26)\n",
      "('Epoch', 77, 'loss:  ', 103998.73)\n",
      "('Epoch', 78, 'loss:  ', 104016.81)\n",
      "('Epoch', 79, 'loss:  ', 103993.35)\n",
      "('Epoch', 80, 'loss:  ', 104007.22)\n",
      "('Epoch', 81, 'loss:  ', 104014.58)\n",
      "('Epoch', 82, 'loss:  ', 104000.8)\n",
      "('Epoch', 83, 'loss:  ', 103984.9)\n",
      "('Epoch', 84, 'loss:  ', 104008.7)\n",
      "('Epoch', 85, 'loss:  ', 104015.02)\n",
      "('Epoch', 86, 'loss:  ', 104007.94)\n",
      "('Epoch', 87, 'loss:  ', 104007.57)\n",
      "('Epoch', 88, 'loss:  ', 103997.16)\n",
      "('Epoch', 89, 'loss:  ', 103994.77)\n",
      "('Epoch', 90, 'loss:  ', 103999.81)\n",
      "('Epoch', 91, 'loss:  ', 103990.47)\n",
      "('Epoch', 92, 'loss:  ', 103995.26)\n",
      "('Epoch', 93, 'loss:  ', 104006.77)\n",
      "('Epoch', 94, 'loss:  ', 103984.4)\n",
      "('Epoch', 95, 'loss:  ', 103998.87)\n",
      "('Epoch', 96, 'loss:  ', 104006.39)\n",
      "('Epoch', 97, 'loss:  ', 104010.08)\n",
      "('Epoch', 98, 'loss:  ', 103984.07)\n",
      "('Epoch', 99, 'loss:  ', 103989.9)\n"
     ]
    }
   ],
   "source": [
    "optimize(num_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = tf.round(Pij * 2) / 2\n",
    "prediction= tf.cast(prediction, tf.float64)\n",
    "correct_prediction = tf.equal(P_target, prediction)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read training data\n",
    "X_val, y_val, Query_val = readDataset('./MSLR-WEB10K/Fold1/vali.txt')\n",
    "\n",
    "# Extract document pairs\n",
    "pairs_val = extractPairsOfRatedSites(y_val, Query_val)\n",
    "X_val_array=np.array(X_val)\n",
    "data1_val,data2_val=separate_training(X_val_array,pairs_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "P_target_val=np.ones([data1_val.shape[0],1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len_test=500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_dict_test = {A: data1_val[0:len_test],B:data2_val[0:len_test] ,P_true: P_target_val[0:len_test]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_accuracy():\n",
    "    # Use TensorFlow to compute the accuracy.\n",
    "    acc = session.run(accuracy, feed_dict=feed_dict_test)\n",
    "    \n",
    "    # Print the accuracy.\n",
    "    print(\"Accuracy on test-set: {0:.1%}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
