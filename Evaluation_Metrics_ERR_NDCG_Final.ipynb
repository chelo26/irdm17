{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import math\n",
    "# import pandas\n",
    "from optparse import OptionParser\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import ensemble\n",
    "from collections import defaultdict\n",
    "from copy import deepcopy\n",
    "from multiprocessing import Pool\n",
    "from itertools import chain\n",
    "import time\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions to extrac the documents, query and rank information\n",
    "def extractFeatures(split):\n",
    "    features = []\n",
    "    for i in range(2, 138):\n",
    "        features.append(float(split[i].split(':')[1]))\n",
    "    # Convert to tuples:\n",
    "    return features\n",
    "\n",
    "def extractQueryData(split):\n",
    "    # Add tuples:\n",
    "    queryFeatures = split[1].split(':')[1]\n",
    "    return queryFeatures\n",
    "\n",
    "def readDataset(path):\n",
    "    dictio_quid= defaultdict(list)\n",
    "    features_list=[]\n",
    "    rank_list=[]\n",
    "    print('Reading training data from file...')\n",
    "    k=0\n",
    "    with open(path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Getting features:\n",
    "            split = line.split()\n",
    "            features=extractFeatures(split)\n",
    "            features_list.append(features)\n",
    "\n",
    "            # Getting the query:\n",
    "            query = int(extractQueryData(split))\n",
    "            #print \"query: \"+str(query)\n",
    "            # Getting rank:\n",
    "            rank = int(split[0])\n",
    "            rank_list.append(rank)\n",
    "\n",
    "            # Feeding dictionary:\n",
    "            dictio_quid[query].append((features, rank))\n",
    "            k+=1\n",
    "            #if k==10:\n",
    "            #   break\n",
    "    print('Number of query ID %d' %(len(features_list)))\n",
    "    return np.array(features_list), np.array(rank_list), dictio_quid\n",
    "\n",
    "# Normalisation:\n",
    "def normalize_features(features):\n",
    "    features=np.array(features)\n",
    "\n",
    "    # Substracting the mean:\n",
    "    mean_features = np.mean(features, axis=0)\n",
    "    features = features - mean_features\n",
    "\n",
    "    # Dividing by the std:\n",
    "    std_features = np.std(features, axis=0)\n",
    "    features = features / std_features\n",
    "    #print \"features normalized\"\n",
    "    return features, mean_features, std_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ORDERING for evaluation:\n",
    "def evaluate(model, dictio_val, mean_Xval, std_Xval):\n",
    "    dictio_evaluation = defaultdict(list)\n",
    "\n",
    "    for key in dictio_val.keys():\n",
    "        temp_list = dictio_val[key]\n",
    "        for features_vec, relevance in temp_list:\n",
    "            # Features:\n",
    "            features_norm = (np.array(features_vec) - mean_Xval) / std_Xval\n",
    "            features_norm = features_norm.reshape(1,-1)\n",
    "            #features_norm = features_norm.reshape(-1,1)\n",
    "\n",
    "            # Prediction:\n",
    "            prediction = model.predict(features_norm)\n",
    "\n",
    "            # Dictionary:\n",
    "            dictio_evaluation[key].append((prediction[0], relevance))\n",
    "\n",
    "            # print features_vec,relevance\n",
    "\n",
    "    return dictio_evaluation\n",
    "def reorder_dictio(dictio_eval):\n",
    "    for key in dictio_eval.keys():\n",
    "        #dictio_eval[key]=sorted(dictio_eval[key], reverse=True,key=lambda tup: (tup[1], tup[0]))\n",
    "        dictio_eval[key] = sorted(dictio_eval[key], reverse=True, key=lambda tup: tup[1])\n",
    "    return dictio_eval\n",
    "\n",
    "def txt_to_dictio_eval(file_name):\n",
    "    \n",
    "    model_output_array = np.loadtxt(file_name)\n",
    "    model_output_arr_rel = np.concatenate((model_output_array, np.reshape(tes_ranks, (-1, 1))), axis=1)\n",
    "    \n",
    "    model_output_dict = defaultdict(list)\n",
    "    model_output_dict_or = defaultdict(list)\n",
    "\n",
    "    for i in model_output_arr_rel:\n",
    "        model_output_dict[i[0]].append((i[3], i[2]))\n",
    "\n",
    "    for i in model_output_dict.keys():\n",
    "        model_output_dict[i].sort(reverse=True)\n",
    "\n",
    "    for i in model_output_dict.keys():\n",
    "        model_output_dict_or[i] = [(x[1] ,x[0]) for x in model_output_dict[i]]\n",
    "        \n",
    "    return model_output_dict_or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NDCG:\n",
    "def dcg_score(y_true, y_score, k=10, gains=\"exponential\"):\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "\n",
    "    if gains == \"exponential\":\n",
    "        gains = 2 ** y_true - 1\n",
    "    elif gains == \"linear\":\n",
    "        gains = y_true\n",
    "    else:\n",
    "        raise ValueError(\"Invalid gains option.\")\n",
    "\n",
    "    # highest rank is 1 so +2 instead of +1\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gains / discounts)\n",
    "\n",
    "def ndcg_score(y_true, y_score, k=10, gains=\"exponential\"):\n",
    "    best = dcg_score(y_true, y_true, k, gains)\n",
    "    actual = dcg_score(y_true, y_score, k, gains)\n",
    "    return actual / best\n",
    "\n",
    "def separate(relevance_tuple):\n",
    "    y_true=[]\n",
    "    y_pred = []\n",
    "    for tup in relevance_tuple:\n",
    "        y_pred.append(tup[0])\n",
    "        y_true.append(tup[1])\n",
    "    return y_true,y_pred\n",
    "\n",
    "def ndcg(dictio_eval):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    b= reorder_dictio(dictio_eval)\n",
    "    new_b=[]\n",
    "    for qid in b.keys():\n",
    "        r_true=[]\n",
    "        r_pred=[]\n",
    "        for i,j in zip(pd.DataFrame(b[qid])[0], pd.DataFrame(b[qid])[1]):\n",
    "            r_pred.append(i)\n",
    "            r_true.append(j)\n",
    "        score=ndcg_score(r_true,r_pred)    \n",
    "        #new_b[qid]= [r_true, r_pred]\n",
    "        new_b.append(score)\n",
    "    return new_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## ERR:\n",
    "\n",
    "def get_bins(dictio_eval):\n",
    "    #Get the bins:\n",
    "    all_tuples=[]\n",
    "    for key in dictio_eval.keys():\n",
    "        all_tuples.append(dictio_eval[key])\n",
    "    all_tuples=list(chain.from_iterable(all_tuples))\n",
    "    all_scores=[i[0] for i in all_tuples]\n",
    "    all_true=[i[1] for i in all_tuples]\n",
    "    number_bins=4#len(set(all_true))\n",
    "    bins = np.histogram(all_scores, bins=number_bins, range=None, normed=False, weights=None)[1]\n",
    "    return bins\n",
    "\n",
    "def assign_to_bin(list_score,bins):\n",
    "    # Get bins\n",
    "    #bins = np.histogram(list_score, bins=number_bins, range=None, normed=False, weights=None)[1]\n",
    "    inds = np.digitize(list_score, bins,right=True)\n",
    "    return inds\n",
    "\n",
    "# Functions to calculate ERR:\n",
    "\n",
    "GAMMA=0.450\n",
    "def get_proba2(list_tuples,bins):\n",
    "    list_proba=[]\n",
    "    list_score = [i[0] for i in list_tuples]\n",
    "    list_true = [i[1] for i in list_tuples]\n",
    "    pred_relevance = assign_to_bin(list_score,bins)\n",
    "    list_tuples=zip(pred_relevance,list_true)\n",
    "    for r_pred,r_true in list_tuples:\n",
    "        proba = ((np.power(2,r_pred))-1)/ np.power(2,4)#np.max(r_pred))\n",
    "        list_proba.append(proba)\n",
    "    return list_proba\n",
    "\n",
    "## Different:\n",
    "def get_proba(list_tuples,bins):\n",
    "    list_proba=[]\n",
    "    list_score = [i[0] for i in list_tuples]\n",
    "#    print list_score\n",
    "    list_true = range(len(list_tuples))\n",
    "#   print list_true\n",
    "    pred_relevance = assign_to_bin(list_score,bins)\n",
    "#    print pred_relevance\n",
    "    list_tuples=zip(pred_relevance,list_true)\n",
    "    for r_pred,r_true in list_tuples:\n",
    "        proba = ((np.power(2,r_pred))-1)/ np.power(2,4)#np.max(r_pred))\n",
    "        list_proba.append(proba)\n",
    "    return list_proba\n",
    "\n",
    "def get_ERR(list_proba,n=10,gamma=0.5):\n",
    "    r=2\n",
    "    err = list_proba[0]\n",
    "    last_proba=1\n",
    "    for i in xrange(1,len(list_proba)):\n",
    "        actual_proba=list_proba[i]\n",
    "        previous_proba=(1-list_proba[i-1])*last_proba\n",
    "        #print proba\n",
    "        stop_proba=actual_proba*previous_proba\n",
    "        err+=stop_proba/r\n",
    "        last_proba=previous_proba\n",
    "        r+=1\n",
    "    return err\n",
    "\n",
    "def ERR(dictio_eval,n=10,gamma=GAMMA):\n",
    "    list_ERR=[]\n",
    "    # Get the bins:\n",
    "    bins=get_bins(dictio_eval)\n",
    "    for key in dictio_eval.keys():\n",
    "        list_tuples=dictio_eval[key]\n",
    "        list_proba=get_proba(list_tuples,bins)\n",
    "        err_result=get_ERR(list_proba,n,gamma)\n",
    "        list_ERR.append(err_result)\n",
    "    return list_ERR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## MAIN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training data from file...\n",
      "Number of query ID 241521\n"
     ]
    }
   ],
   "source": [
    "tes_features, tes_ranks, tes_queries= readDataset('./MSLR-WEB10K/Fold1/test.txt')\n",
    "tes_features, mean_Xval, std_Xval= normalize_features(tes_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the list of scores:\n",
    "import os\n",
    "list_scores=[]\n",
    "for file in os.listdir(\"./best_models/\"):\n",
    "    if file.endswith(\".txt\"):\n",
    "        list_scores.append(os.path.join(\"./best_models/\", file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./best_models/test_LM_rank_score_LambdaMART1.txt',\n",
       " './best_models/test_RB_rank_score_RB_model_r100_tc10.txt',\n",
       " './best_models/test_RN_rank_score_RN_model_e20_l2_n1_lr5e-05.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_names=[\"LambdaMart\",\"RankBoost\",\"RankNet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Chelo/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:20: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.46181678515272756, 0.47330885960864894)\n",
      "(0.37704545135360484, 0.50873371130849543)\n",
      "(0.23251493220839448, 0.40672594862600914)\n"
     ]
    }
   ],
   "source": [
    "# Extract\n",
    "dict_scores=defaultdict(list)\n",
    "gammas=[0.65,0.35,0.25]\n",
    "i=0\n",
    "for best_score,key in zip(list_scores,model_names):\n",
    "    dictio_eval = txt_to_dictio_eval(best_score)\n",
    "    lst_ndcg = ndcg(dictio_eval)\n",
    "    lst_ERR = ERR(dictio_eval,gamma=gammas[i])\n",
    "    print (np.nanmean(lst_ndcg), np.nanmean(lst_ERR))\n",
    "    dict_scores[key].append(np.nanmean(lst_ndcg))\n",
    "    dict_scores[key].append(np.nanmean(lst_ERR))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "query=58\n",
    "dict_query58=defaultdict(list)\n",
    "for best_score,key in zip(list_scores,model_names):\n",
    "    dictio_eval = txt_to_dictio_eval(best_score)\n",
    "    list_tuples = dictio_eval[query][0:10]\n",
    "    dict_query58[key].append(list_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.39973296644484663, 4.0)\n",
      "(0.39973296644484663, 4.0)\n",
      "(0.39973296644484663, 3.0)\n",
      "(0.39973296644484663, 3.0)\n",
      "(0.42531100212049794, 2.0)\n",
      "(0.39973296644484663, 2.0)\n",
      "(0.39973296644484663, 1.0)\n",
      "(0.39973296644484663, 1.0)\n",
      "(0.39973296644484663, 1.0)\n",
      "(0.39973296644484663, 1.0)\n"
     ]
    }
   ],
   "source": [
    "for i in dict_query58[\"RankNet\"][0]:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "query=58\n",
    "rank_dict=defaultdict(list)\n",
    "for score,key in zip(list_scores,model_names):\n",
    "    dictio_eval = txt_to_dictio_eval(score)\n",
    "    # Get first 10 results:\n",
    "    list_tuples = dictio_eval[query][0:10]\n",
    "    list_score = [i[0] for i in list_tuples]\n",
    "    list_true = [i[1] for i in list_tuples]\n",
    "    bins=get_bins(dictio_eval)\n",
    "    # Get predicted relevance:\n",
    "    pred_relevance = assign_to_bin(list_score,bins)\n",
    "    list_tuples=zip(pred_relevance,list_true)\n",
    "    rank_dict[key].append(list_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'LambdaMart': [[(3, 4.0),\n",
       "               (2, 4.0),\n",
       "               (3, 3.0),\n",
       "               (2, 3.0),\n",
       "               (2, 2.0),\n",
       "               (2, 2.0),\n",
       "               (2, 1.0),\n",
       "               (2, 1.0),\n",
       "               (2, 1.0),\n",
       "               (2, 1.0)]],\n",
       "             'RankBoost': [[(3, 4.0),\n",
       "               (1, 4.0),\n",
       "               (2, 3.0),\n",
       "               (1, 3.0),\n",
       "               (2, 2.0),\n",
       "               (1, 2.0),\n",
       "               (2, 1.0),\n",
       "               (2, 1.0),\n",
       "               (2, 1.0),\n",
       "               (2, 1.0)]],\n",
       "             'RankNet': [[(0, 4.0),\n",
       "               (0, 4.0),\n",
       "               (0, 3.0),\n",
       "               (0, 3.0),\n",
       "               (4, 2.0),\n",
       "               (0, 2.0),\n",
       "               (0, 1.0),\n",
       "               (0, 1.0),\n",
       "               (0, 1.0),\n",
       "               (0, 1.0)]]})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rank_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "table_best=pd.DataFrame.from_dict(dict_scores,\"index\")\n",
    "table_best.columns=[\"NDCG@10\",\"ERR@10\"]\n",
    "table_best.to_csv(\"final_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
