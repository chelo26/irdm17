{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Q1 RankNet\n",
    "\n",
    "Pairwise Ranking NN\n",
    "Requires 'processed_array.gz.npy'. This is formatted badly but works for the functions in this notebook\n",
    "Currently set for only 2 qid's ('166', '142') but can be increased. 'n_qid' must be increased to the number of query Todo- map qid to a interger between 0 an #qid to save space in weights matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "import tensorflow as tf \n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_load = np.load('processed_array.gz.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def select_qid(data, qid1, qid2):\n",
    "    # Select inst for specified qid, eg '116'\n",
    "    \n",
    "    data_sub = []\n",
    "    for i in range(len(data)-1):\n",
    "        if data[i][1] == qid1 or data[i][1] == qid2:\n",
    "            data_sub.append(data[i])\n",
    "    print('Qid:', qid, np.asarray(data_sub).shape)\n",
    "    return data_sub\n",
    "\n",
    "def normalise_feats(data_sub):\n",
    "    # Standardise features (per column) by -mean and /SD\n",
    "    # Replace rel and qid with unnormalised\n",
    "    \n",
    "    data_sub = np.asarray(data_sub)\n",
    "    data_sub_norm = preprocessing.scale(data_sub, axis=1)\n",
    "    data_sub_norm[:, 0] = data_sub[:, 0]\n",
    "    data_sub_norm[:, 1] = data_sub[:, 1]\n",
    "    return data_sub_norm\n",
    "\n",
    "def generate_pairs(data_sub):\n",
    "    # Generate pairs of non-matching relievence. Higher relevence 1st, lower 2nd\n",
    "    \n",
    "    pair_sets= []\n",
    "    pair_y= []\n",
    "    for i in data_sub:\n",
    "        for j in data_sub:\n",
    "            if i[1]== j[1]:\n",
    "                pair_sets.append((i, j))\n",
    "                if i[0]> j[0]:\n",
    "                    pair_y.append([1])\n",
    "                if i[0]== int(j[0]):\n",
    "                    pair_y.append([0.5])\n",
    "                if i[0]< j[0]:\n",
    "                    pair_y.append([0])\n",
    "                \n",
    "    return np.asarray(pair_sets), np.asarray(pair_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qid: Tensor(\"Cast_2:0\", shape=(), dtype=int32) (214, 138)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jamesshields/anaconda/lib/python3.5/site-packages/sklearn/utils/validation.py:420: DataConversionWarning: Data with input dtype <U16 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "24698"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_166 = select_qid(data_load, '166', '142')\n",
    "data_166_n = normalise_feats(data_166)\n",
    "x_116, y_166 = generate_pairs(data_166_n)\n",
    "len(x_116)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_pair = tf.placeholder(\"float\", [None, 2, 138])\n",
    "y_gold = tf.placeholder(\"float\", [None, 1])\n",
    "\n",
    "x_feat1 = tf.slice(x_pair, [0, 0, 2], [-1, 1, 136])\n",
    "x_feat2 = tf.slice(x_pair, [0, 1, 2], [-1, 1, 136])\n",
    "x_feat1 = tf.reshape(x_feat1, [-1, 136])\n",
    "x_feat2 = tf.reshape(x_feat2, [-1, 136])\n",
    "qid = tf.cast(x_pair[0,0,1], dtype='int32')\n",
    "d_qid = qid- 1\n",
    "\n",
    "d_in = 136\n",
    "d_hidden = 500\n",
    "d_out = 1\n",
    "n_qid = 166\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([n_qid, d_in, d_hidden], mean= 0.01, stddev= 0.1))\n",
    "b1 = tf.Variable(tf.random_normal([n_qid, d_hidden], mean= 0.01, stddev= 0.1))\n",
    "W2 = tf.Variable(tf.random_normal([n_qid, d_hidden, d_out], mean= 0.01, stddev= 0.1))\n",
    "b2 = tf.Variable(tf.random_normal([n_qid, d_out], mean= 0.01, stddev= 0.1))\n",
    "\n",
    "a1_i = tf.matmul(x_feat1, W1[d_qid, :, :])+ b1[d_qid, :]\n",
    "z1_i = tf.sigmoid(a1_i)\n",
    "o_i = tf.matmul(z1_i, W2[d_qid, :, :])+ b2[d_qid, :]\n",
    "\n",
    "a1_j = tf.matmul(x_feat2, W1[d_qid, :, :])+ b1[d_qid, :]\n",
    "z1_j = tf.sigmoid(a1_j)\n",
    "o_j = tf.matmul(z1_j, W2[d_qid, :, :])+ b2[d_qid, :]\n",
    "\n",
    "o_ij = o_i- o_j\n",
    "P_ij = tf.exp(o_ij)/ (1+ tf.exp(o_ij))\n",
    "\n",
    "#C_ij = -P_ij* tf.log(P_ij) - (1- P_ij)* tf.log(1- P_ij)\n",
    "cross_entropy = -tf.reduce_sum(y_gold* tf.log(tf.clip_by_value(P_ij, 1e-10,1.0)))\n",
    "optimiser = tf.train.GradientDescentOptimizer(0.015).minimize(cross_entropy)\n",
    "\n",
    "prediction = tf.round(P_ij * 2) / 2\n",
    "mistakes = tf.not_equal(y_gold, prediction)\n",
    "accuracy = 1- tf.reduce_mean(tf.cast(mistakes, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166\n"
     ]
    }
   ],
   "source": [
    "# Variable dev\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    init_dict= {x_pair: x_116[:100],\n",
    "                y_gold: y_166[:100]}\n",
    "\n",
    "    print(sess.run(qid, feed_dict= init_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Init accuracy: 0.33\n",
      "Epoch 0 loss: 5935.0181511\n",
      "Epoch 1 loss: 6840.7761392\n",
      "Epoch 2 loss: 6996.24825177\n",
      "Epoch 3 loss: 7080.07737792\n",
      "Epoch 4 loss: 7121.56140922\n",
      "Epoch 5 loss: 7181.3146451\n",
      "Epoch 6 loss: 7235.83302364\n",
      "Epoch 7 loss: 7282.1647341\n",
      "Epoch 8 loss: 7338.92982009\n",
      "Epoch 9 loss: 7369.24999267\n",
      "Trained accuracy: 0.34\n",
      "Test accuracy: 0.47\n"
     ]
    }
   ],
   "source": [
    "# Train and test\n",
    "batch_sz= 30\n",
    "iter_= int(x_116.shape[0]/ batch_sz)\n",
    "epoch= 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    init_dict= {x_pair: x_116[:100],\n",
    "                y_gold: y_166[:100]}\n",
    "    print('Init accuracy:', (sess.run(accuracy, feed_dict= init_dict)))\n",
    "    \n",
    "    for e in range(epoch):\n",
    "        e_loss= 0\n",
    "        for i in range(iter_):\n",
    "            i_loss= 0\n",
    "            sta= i* batch_sz\n",
    "            end= (i+ 1)* batch_sz\n",
    "            iter_dict= {x_pair: x_116[sta: end],\n",
    "                        y_gold: y_166[sta: end]}\n",
    "            sess.run(optimiser, feed_dict= iter_dict)\n",
    "            e_loss+= sess.run(cross_entropy, feed_dict= iter_dict)\n",
    "        if e% (epoch/ 10)== 0:\n",
    "            print('Epoch', e, 'loss:', e_loss)\n",
    "        \n",
    "    print('Trained accuracy:', (sess.run(accuracy, feed_dict= init_dict)))\n",
    "    \n",
    "    test_dict= {x_pair: x_116[100:1000],\n",
    "                y_gold: y_166[100:1000]}    \n",
    "    print('Test accuracy:' ,(sess.run(accuracy, feed_dict= test_dict)))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
